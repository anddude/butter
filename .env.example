# server runs here
PORT=3000


# word frequency / safety limits
MAX_TOP_LIMIT=12
# max for retrieved RAG context to keep prompts cheaper + faster
MAX_CONTEXT_CHARS=3500
# cap user text length
MAX_INPUT_CHARS=20000


# llm selection
LLM_PROVIDER=openai


# OpenAI (paid)  -->  uses OPENAI_API_KEY + OPENAI_MODEL
OPENAI_API_KEY=replace_me
# chaepest paid model
OPENAI_MODEL=gpt-5-nano


# OpenRouter (free)  -->  uses OPENROUTER_API_KEY + OPENROUTER_MODEL + OPENROUTER_BASE_URL
#    -  is OpenAI-compatible  -->  using OpenAI SDK with a different base url
OPENROUTER_API_KEY=replace_me_if_using_openrouter
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# example free model
OPENROUTER_MODEL=meta-llama/llama-3.3-70b-instruct:free


# local embeddings used in BOTH modes

# transformers.js model id  -->  allows RAG pipleine to run locally in the client (browser)
EMBED_MODEL=Xenova/all-MiniLM-L6-v2

# pinecone (vector db)
PINECONE_API_KEY=replace_me
PINECONE_INDEX=butter-kb
PINECONE_NAMESPACE=dev
